{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "import numpy as np\n",
    "\n",
    "tfe.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "1/1 [==============================] - 1s 898ms/step - loss: 1.6069\n",
      "Epoch 2/120\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 1.6069\n",
      "Epoch 3/120\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.6069\n",
      "Epoch 4/120\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.6068\n",
      "Epoch 5/120\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.6068\n",
      "Epoch 6/120\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.6067\n",
      "Epoch 7/120\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.6067\n",
      "Epoch 8/120\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.6067\n",
      "Epoch 9/120\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.6066\n",
      "Epoch 10/120\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.6066\n",
      "Epoch 11/120\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.6065\n",
      "Epoch 12/120\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.6065\n",
      "Epoch 13/120\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.6064\n",
      "Epoch 14/120\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.6064\n",
      "Epoch 15/120\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.6063\n",
      "Epoch 16/120\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.6062\n",
      "Epoch 17/120\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.6062\n",
      "Epoch 18/120\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.6061\n",
      "Epoch 19/120\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.6060\n",
      "Epoch 20/120\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.6060\n",
      "Epoch 21/120\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.6059\n",
      "Epoch 22/120\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.6058\n",
      "Epoch 23/120\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.6057\n",
      "Epoch 24/120\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.6056\n",
      "Epoch 25/120\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.6055\n",
      "Epoch 26/120\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.6054\n",
      "Epoch 27/120\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.6053\n",
      "Epoch 28/120\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.6052\n",
      "Epoch 29/120\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.6051\n",
      "Epoch 30/120\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.6049\n",
      "Epoch 31/120\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.6048\n",
      "Epoch 32/120\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.6047\n",
      "Epoch 33/120\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.6045\n",
      "Epoch 34/120\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.6044\n",
      "Epoch 35/120\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.6042\n",
      "Epoch 36/120\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.6040\n",
      "Epoch 37/120\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.6038\n",
      "Epoch 38/120\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.6037\n",
      "Epoch 39/120\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.6035\n",
      "Epoch 40/120\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.6032\n",
      "Epoch 41/120\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.6030\n",
      "Epoch 42/120\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.6028\n",
      "Epoch 43/120\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.6025\n",
      "Epoch 44/120\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.6023\n",
      "Epoch 45/120\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.6020\n",
      "Epoch 46/120\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.6017\n",
      "Epoch 47/120\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.6014\n",
      "Epoch 48/120\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.6011\n",
      "Epoch 49/120\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.6008\n",
      "Epoch 50/120\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.6004\n",
      "Epoch 51/120\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.6000\n",
      "Epoch 52/120\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.5996\n",
      "Epoch 53/120\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.5992\n",
      "Epoch 54/120\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.5988\n",
      "Epoch 55/120\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.5983\n",
      "Epoch 56/120\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.5978\n",
      "Epoch 57/120\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.5973\n",
      "Epoch 58/120\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.5967\n",
      "Epoch 59/120\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.5962\n",
      "Epoch 60/120\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.5955\n",
      "Epoch 61/120\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.5949\n",
      "Epoch 62/120\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.5942\n",
      "Epoch 63/120\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.5935\n",
      "Epoch 64/120\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.5927\n",
      "Epoch 65/120\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.5919\n",
      "Epoch 66/120\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.5910\n",
      "Epoch 67/120\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.5901\n",
      "Epoch 68/120\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.5891\n",
      "Epoch 69/120\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.5881\n",
      "Epoch 70/120\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.5870\n",
      "Epoch 71/120\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.5858\n",
      "Epoch 72/120\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.5845\n",
      "Epoch 73/120\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.5832\n",
      "Epoch 74/120\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.5817\n",
      "Epoch 75/120\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.5802\n",
      "Epoch 76/120\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.5785\n",
      "Epoch 77/120\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.5767\n",
      "Epoch 78/120\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.5748\n",
      "Epoch 79/120\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.5727\n",
      "Epoch 80/120\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.5705\n",
      "Epoch 81/120\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.5681\n",
      "Epoch 82/120\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.5654\n",
      "Epoch 83/120\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.5626\n",
      "Epoch 84/120\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.5594\n",
      "Epoch 85/120\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.5560\n",
      "Epoch 86/120\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.5522\n",
      "Epoch 87/120\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.5481\n",
      "Epoch 88/120\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.5435\n",
      "Epoch 89/120\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.5385\n",
      "Epoch 90/120\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.5329\n",
      "Epoch 91/120\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.5266\n",
      "Epoch 92/120\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.5197\n",
      "Epoch 93/120\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.5119\n",
      "Epoch 94/120\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.5031\n",
      "Epoch 95/120\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.4933\n",
      "Epoch 96/120\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.4821\n",
      "Epoch 97/120\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.4696\n",
      "Epoch 98/120\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.4554\n",
      "Epoch 99/120\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.4393\n",
      "Epoch 100/120\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.4210\n",
      "Epoch 101/120\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.4004\n",
      "Epoch 102/120\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.3771\n",
      "Epoch 103/120\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.3508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 104/120\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.3212\n",
      "Epoch 105/120\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.2881\n",
      "Epoch 106/120\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.2513\n",
      "Epoch 107/120\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.2107\n",
      "Epoch 108/120\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.1661\n",
      "Epoch 109/120\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.1177\n",
      "Epoch 110/120\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.0657\n",
      "Epoch 111/120\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.0104\n",
      "Epoch 112/120\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.9526\n",
      "Epoch 113/120\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.8929\n",
      "Epoch 114/120\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.8321\n",
      "Epoch 115/120\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.7712\n",
      "Epoch 116/120\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.7109\n",
      "Epoch 117/120\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6521\n",
      "Epoch 118/120\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5953\n",
      "Epoch 119/120\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5410\n",
      "Epoch 120/120\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4894\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xfceb320>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N, D, W, H = 10, 20, 30, 40\n",
    "word_to_idx = {'<NULL>': 0, 'cat': 2, 'dog': 3, '<START>': 4, '<END>': 5}\n",
    "V = len(word_to_idx)\n",
    "T = 13\n",
    "\n",
    "features = np.linspace(-0.5, 1.7, num=N*D).reshape(N, D).astype(np.float32)\n",
    "captions = (np.arange(N * T) % V).reshape(N, T).astype(np.int32)\n",
    "\n",
    "input_features = tf.keras.layers.Input(shape=(D,), dtype=tf.float32)\n",
    "project_layer = tf.keras.layers.Dense(units=H)\n",
    "h0 = project_layer(input_features)\n",
    "\n",
    "input_captions = tf.keras.layers.Input(shape=(None, T-1), dtype=tf.int32)\n",
    "w_embedding = tf.keras.layers.Embedding(V, W, input_length=T-1, embeddings_initializer=tf.keras.initializers.RandomNormal())\n",
    "output_x = w_embedding(input_captions)\n",
    "\n",
    "input_state_c = tf.keras.layers.Input(shape=(H,), dtype=tf.float32)\n",
    "decoder_initial_state = [h0, input_state_c]\n",
    "decoder = tf.keras.layers.CuDNNLSTM(units=H, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder(output_x, initial_state=decoder_initial_state)\n",
    "\n",
    "dense_layer = tf.keras.layers.Dense(units=V, activation='softmax')\n",
    "scores = dense_layer(decoder_outputs)\n",
    "# print(captions[:, 1:].shape)  # shpae = (10, 12)\n",
    "# tf.one_hot(captions[:, 1:], depth=V)\n",
    "\n",
    "captioning_model = tf.keras.Model(inputs=[input_features, input_captions, input_state_c], outputs=scores)\n",
    "captioning_model.compile(optimizer=tf.train.RMSPropOptimizer(0.005), loss='categorical_crossentropy')\n",
    "captioning_model.fit(x=[features, captions[:, :-1], np.zeros((features.shape[0], H), dtype=np.float32)], \n",
    "                     y=tf.one_hot(captions[:, 1:], depth=V, dtype=tf.float32), epochs=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_start = tf.keras.layers.Input(shape=(None, 1), dtype=tf.int32)\n",
    "output_start = w_embedding(input_start)\n",
    "\n",
    "sampling_w_embeded_model = tf.keras.Model(inputs=input_start, outputs=output_start)\n",
    "sampling_proj_model = tf.keras.Model(inputs=input_features, outputs=h0)\n",
    "\n",
    "input_state_h = tf.keras.layers.Input(shape=(H,), dtype=tf.float32)\n",
    "sampling_initial_state = [input_state_h, input_state_c]\n",
    "\n",
    "sampling_initial_state\n",
    "sampling_lstm_input = tf.keras.layers.Input(shape=(None, W))\n",
    "sampling_lstm_output, sampling_h, sampling_c = decoder(sampling_lstm_input, initial_state=sampling_initial_state)\n",
    "sampling_output_state = [sampling_h, sampling_c]\n",
    "sampling_dense_output = dense_layer(sampling_lstm_output)\n",
    "\n",
    "sampling_decoder = tf.keras.Model(inputs=[sampling_lstm_input]+sampling_initial_state, outputs=[sampling_dense_output]+sampling_output_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def sampling_caption(input_features, max_length=30):\n",
    "    N = features.shape[0]\n",
    "    \n",
    "    start_embed = tf.convert_to_tensor(word_to_idx['<START>'] * np.ones(N, dtype=np.int32))\n",
    "    input_x = sampling_w_embeded_model.predict(start_embed)\n",
    "    initial_h = sampling_proj_model.predict(input_features)\n",
    "    \n",
    "    input_state = [initial_h, np.zeros_like(initial_h)]\n",
    "    \n",
    "    captions = (word_to_idx['<NULL>'] * np.ones((N, max_length))).astype(np.int32)\n",
    "    \n",
    "    # cur_dense_output, cur_state_h, cur_state_c = sampling_decoder.predict([input_x]+input_state)\n",
    "    # print(cur_state_h)\n",
    "    # print(input_x.shape)\n",
    "    \n",
    "    for t in range(max_length):\n",
    "        cur_dense_output, cur_state_h, cur_state_c = sampling_decoder.predict([input_x]+input_state)\n",
    "        \n",
    "        # print(\"the shape is\")\n",
    "        # print(captions[:, t].shape)\n",
    "        captions[:, t] += tf.squeeze(tf.cast(tf.argmax(cur_dense_output, axis=2), tf.int32))\n",
    "        input_x = sampling_w_embeded_model.predict(captions[:, t])\n",
    "        input_state = [cur_state_h, cur_state_c]\n",
    "    \n",
    "    return captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "captions dtype\n",
      "int32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dtype('int32')"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampling_caption(features).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=54130, shape=(2, 4, 3), dtype=float32, numpy=\n",
       "array([[[ 0.09987143, -0.03769374, -0.03912736],\n",
       "        [ 0.02454221,  0.06779458,  0.06717668],\n",
       "        [-0.01099924, -0.06941702, -0.03229584],\n",
       "        [ 0.00574606, -0.08676705, -0.01706604]],\n",
       "\n",
       "       [[ 0.00574606, -0.08676705, -0.01706604],\n",
       "        [-0.01099924, -0.06941702, -0.03229584],\n",
       "        [ 0.09987143, -0.03769374, -0.03912736],\n",
       "        [ 0.02454221,  0.06779458,  0.06717668]]], dtype=float32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.asarray([[0, 3, 1, 2], [2, 1, 0, 3]])\n",
    "input_x = tf.keras.layers.Input(shape=(None, 4))\n",
    "w_embedding = tf.keras.layers.Embedding(5, 3, input_length=T, embeddings_initializer=tf.keras.initializers.RandomNormal())\n",
    "# out = w_embedding(input_x)\n",
    "# model = tf.keras.Model(inputs=[input_x], outputs=out)\n",
    "# model.apply(tf.convert_to_tensor(x, dtype=tf.int32))\n",
    "w_embedding(tf.convert_to_tensor(x, dtype=tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
